{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import gc\n",
    "sc = SparkContext(master = 'local')\n",
    "spark = SparkSession.builder.appName('pySpark proj3_test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, HashingTF\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 17:22:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# spark setup\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(master = 'local')\n",
    "spark = SparkSession.builder.appName('pySpark word-count').config('spark.some.config.option', 'some-value')             .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "data = spark.read.format(\"csv\").option(\"header\",True).load(\"/Users/iris/spam_data.csv\") \\\n",
    "    .withColumnRenamed(\"Category\", \"label\") \\\n",
    "    .withColumnRenamed(\"Message\", \"text\")\n",
    "data_df = data.toDF(\"label\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "5574 ['label', 'text']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.show(3)\n",
    "print(data_df.count(), data_df.columns)\n",
    "type(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "# replace label spam = 0 and ham = 1\n",
    "data_df = data_df.withColumn('label', regexp_replace('label', 'spam', '0'))\n",
    "data_df = data_df.withColumn('label', regexp_replace('label', 'ham', '1'))\n",
    "\n",
    "# cast those string categories as int dtype\n",
    "data_df = data_df.withColumn('label', data_df.label.cast(IntegerType()))\n",
    "\n",
    "print(data_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pre-processing\n",
    "# post_data = Tokenizer(inputCol='text', outputCol='words').transform(data_df)\n",
    "\n",
    "# #test_post_data = RegexTokenizer(inputCol='text', outputCol='token', pattern='\\\\W+')\n",
    "\n",
    "# #countTokens = udf(lambda x: len(x), IntegerType())\n",
    "# post_data.show(4)\n",
    "# post_data.columns\n",
    "# from pyspark.sql.functions import col \n",
    "# post_data.select('label', 'text', 'words').withColumn('token', countTokens(col('words'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    1|Go until jurong p...|\n",
      "|    1|Ok lar... Joking ...|\n",
      "|    0|Free entry in 2 a...|\n",
      "|    1|U dun say so earl...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "| null| give me a call i...|\n",
      "| null| DEVIOUSBITCH.ANYWAY|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data_df.filter(data_df.label.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572 ['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df.dropna()\n",
    "print(data_df.count(), data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_set, testing_set = data_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "#tokenise\n",
    "# 1. clean data and tokenize sentences using RegexTokenizer\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# 2. hashTF the data\n",
    "hasherTF = HashingTF(inputCol='words', outputCol='features')\n",
    "\n",
    "# 3. classifier\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline = Pipeline(stages = [tokenizer, hasherTF, lr])\n",
    "model_fit = pipeline.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "training_predictions = model_fit.transform(training_set)\n",
    "test_predictions = model_fit.transform(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e04e036266f305a312129840b06cfb353c1d2d005428a9f350cd8fe1c5454b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
