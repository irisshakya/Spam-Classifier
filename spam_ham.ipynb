{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, NGram, HashingTF, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/24 16:09:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# spark setup\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(master = 'local')\n",
    "spark = SparkSession.builder.appName('pySpark word-count').config('spark.some.config.option', 'some-value')             .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "data = spark.read.format(\"csv\").option(\"header\",True).load(\"/Users/iris/spam_data.csv\") \\\n",
    "    .withColumnRenamed(\"Category\", \"label\") \\\n",
    "    .withColumnRenamed(\"Message\", \"text\")\n",
    "data_df = data.toDF(\"label\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               label|count|\n",
      "+--------------------+-----+\n",
      "|ham\\tHI BABE UAWA...|    1|\n",
      "|                 ham| 4825|\n",
      "|                spam|  747|\n",
      "|           ham\\tYeah|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "5574 ['label', 'text']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.show(3)\n",
    "print(data_df.count(), data_df.columns)\n",
    "type(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "# replace label spam = 0 and ham = 1\n",
    "data_df = data_df.withColumn('label', regexp_replace('label', 'spam', '0'))\n",
    "data_df = data_df.withColumn('label', regexp_replace('label', 'ham', '1'))\n",
    "\n",
    "# cast those string categories as int dtype\n",
    "data_df = data_df.withColumn('label', data_df.label.cast(IntegerType()))\n",
    "\n",
    "print(data_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pre-processing\n",
    "# post_data = Tokenizer(inputCol='text', outputCol='words').transform(data_df)\n",
    "\n",
    "# #test_post_data = RegexTokenizer(inputCol='text', outputCol='token', pattern='\\\\W+')\n",
    "\n",
    "# #countTokens = udf(lambda x: len(x), IntegerType())\n",
    "# post_data.show(4)\n",
    "# post_data.columns\n",
    "# from pyspark.sql.functions import col \n",
    "# post_data.select('label', 'text', 'words').withColumn('token', countTokens(col('words'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    1|Go until jurong p...|\n",
      "|    1|Ok lar... Joking ...|\n",
      "|    0|Free entry in 2 a...|\n",
      "|    1|U dun say so earl...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.filter(data_df.label.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 4825|\n",
      "|    0|  747|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropNULL we need to delete empty cells\n",
    "data_df = data_df.dropna()\n",
    "data_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. tokenize and clean data sentences using Tokenizer\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "tokenized_df = tokenizer.transform(data_df)\n",
    "\n",
    "# 2. BiGram the words\n",
    "bigrams = NGram(n=2, inputCol='words', outputCol='bigrams')\n",
    "bigramed_df = bigrams.transform(tokenized_df)\n",
    "\n",
    "# 3. hashTF the data\n",
    "hasherTF = HashingTF(inputCol='bigrams', outputCol='hasher', numFeatures=6000)\n",
    "hashed_df = hasherTF.transform(bigramed_df)\n",
    "\n",
    "# 4. IDF\n",
    "idf = IDF(inputCol='hasher', outputCol='features')\n",
    "idf_df = idf.fit(hashed_df)\n",
    "output_df = idf_df.transform(hashed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(6000,[531,745,10...|    1|\n",
      "|(6000,[2226,2343,...|    1|\n",
      "|(6000,[223,245,33...|    0|\n",
      "|(6000,[1131,1650,...|    1|\n",
      "|(6000,[556,2043,2...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df = output_df.select('features', 'label')\n",
    "output_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1 = output_df.randomSplit([0.8, 0.2], seed=666)\n",
    "train_2, test_2 = output_df.randomSplit([0.8, 0.2], seed=768)\n",
    "train_3, test_3 = output_df.randomSplit([0.8, 0.2], seed=563)\n",
    "train_4, test_4 = output_df.randomSplit([0.8, 0.2], seed=356)\n",
    "train_5, test_5 = output_df.randomSplit([0.8, 0.2], seed=879)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+--------------+--------------------+----------+\n",
      "|    features|label| rawPrediction|         probability|prediction|\n",
      "+------------+-----+--------------+--------------------+----------+\n",
      "|(6000,[],[])|    1|[439.0,3851.0]|[0.10233100233100...|       1.0|\n",
      "|(6000,[],[])|    1|[439.0,3851.0]|[0.10233100233100...|       1.0|\n",
      "|(6000,[],[])|    1|[439.0,3851.0]|[0.10233100233100...|       1.0|\n",
      "|(6000,[],[])|    1|[439.0,3851.0]|[0.10233100233100...|       1.0|\n",
      "|(6000,[],[])|    1|[439.0,3851.0]|[0.10233100233100...|       1.0|\n",
      "+------------+-----+--------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train test\n",
    "dt_clf_1 = DecisionTreeClassifier(featuresCol='features', labelCol=\"label\").fit(train_1)\n",
    "dt_pred_1 = dt_clf_1.transform(test_1)\n",
    "dt_pred_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy') \\\n",
    "                .evaluate(dt_pred_1)\n",
    "check_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedPrecision')   \\\n",
    "                .evaluate(dt_pred_1)\n",
    "check_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedRecall') \\\n",
    "                .evaluate(dt_pred_1)\n",
    "check_f = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')  \\\n",
    "                .evaluate(dt_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8863\n",
      "precision: 0.8780\n",
      "recall: 0.8863\n",
      "F1 score: 0.8571\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: %0.4f' % check_accuracy)\n",
    "print('precision: %0.4f' % check_precision)\n",
    "print('recall: %0.4f' % check_recall)\n",
    "print('F1 score: %0.4f' % check_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6000, {80: 0.0071, 306: 0.0074, 366: 0.0074, 394: 0.0078, 477: 0.0077, 520: 0.0259, 536: 0.0083, 1600: 0.1145, 2289: 0.0094, 2623: 0.0224, 2852: 0.2077, 4454: 0.0157, 4622: 0.1134, 5774: 0.2344, 5984: 0.2108})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf_1.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from pyspark.ml.classification import RandomForestClassifier\n",
    "# # rf = RandomForestClassifier(labelCol='label', featuresCol='')\n",
    "# eval = BinaryClassificationEvaluator()\n",
    "\n",
    "# training_accuracy = eval.evaluate(training_predictions)\n",
    "# # print('Training set accuracy: {:.4g}.'.format(training_accuracy))\n",
    "# test_accuracy = eval.evaluate(test_predictions)\n",
    "# print('Test accuracy: {:.4g}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_1: 0.8648\n",
      "CV_2: 0.8710\n",
      "CV_3: 0.8621\n",
      "CV_4: 0.8683\n",
      "CV_5: 0.8658\n"
     ]
    }
   ],
   "source": [
    "# create Cross-Validator and ParamGrid\n",
    "# pipeline = Pipeline(stages = [tokenizer, bigrams, hasherTF, idf])\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# Run k-fold Cross-Validator \n",
    "cv = CrossValidator(estimator=DecisionTreeClassifier(),\\\n",
    "     estimatorParamMaps=params,\\\n",
    "     evaluator=MulticlassClassificationEvaluator(), \\\n",
    "     seed=645,numFolds=5)\n",
    "cv_model_1 = cv.fit(train_1) \n",
    "cv_model_2 = cv.fit(train_2) \n",
    "cv_model_3 = cv.fit(train_3) \n",
    "cv_model_4 = cv.fit(train_4) \n",
    "cv_model_5 = cv.fit(train_5) \n",
    "\n",
    "print('CV_1: %0.4f' %cv_model_1.avgMetrics[0])\n",
    "print('CV_2: %0.4f' %cv_model_2.avgMetrics[0])\n",
    "print('CV_3: %0.4f' %cv_model_3.avgMetrics[0])\n",
    "print('CV_4: %0.4f' %cv_model_4.avgMetrics[0])\n",
    "print('CV_5: %0.4f' %cv_model_5.avgMetrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction model\n",
    "cv_prediction_1 = cv_model_1.transform(test_1)\n",
    "cv_prediction_1 = cv_prediction_1.select('label', 'prediction')\n",
    "\n",
    "cv_prediction_2 = cv_model_2.transform(test_2)\n",
    "cv_prediction_2 = cv_prediction_2.select('label', 'prediction')\n",
    "\n",
    "cv_prediction_3 = cv_model_3.transform(test_3)\n",
    "cv_prediction_3 = cv_prediction_3.select('label', 'prediction')\n",
    "\n",
    "cv_prediction_4 = cv_model_3.transform(test_4)\n",
    "cv_prediction_4 = cv_prediction_4.select('label', 'prediction')\n",
    "\n",
    "cv_prediction_5 = cv_model_4.transform(test_5)\n",
    "cv_prediction_5 = cv_prediction_5.select('label', 'prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|    8|\n",
      "|    0|       0.0|   33|\n",
      "|    1|       1.0|  934|\n",
      "|    0|       1.0|  116|\n",
      "+-----+----------+-----+\n",
      "\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|    5|\n",
      "|    0|       0.0|   31|\n",
      "|    1|       1.0|  982|\n",
      "|    0|       1.0|  144|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_prediction_1.groupBy('label','prediction').count().show()\n",
    "pred_1_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedPrecision')\n",
    "pred_1_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedRecall')\n",
    "precision_1 = pred_1_precision.evaluate(cv_prediction_1)\n",
    "recall_1 = pred_1_recall.evaluate(cv_prediction_1)\n",
    "\n",
    "cv_prediction_2.groupBy('label','prediction').count().show()\n",
    "pred_2_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedPrecision')\n",
    "pred_2_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedRecall')\n",
    "precision_2 = pred_2_precision.evaluate(cv_prediction_2)\n",
    "recall_2 = pred_2_recall.evaluate(cv_prediction_2)\n",
    "\n",
    "pred_3_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedPrecision')\n",
    "pred_3_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedRecall')\n",
    "precision_3 = pred_3_precision.evaluate(cv_prediction_3)\n",
    "recall_3 = pred_3_recall.evaluate(cv_prediction_3)\n",
    "\n",
    "pred_4_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedPrecision')\n",
    "pred_4_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedRecall')\n",
    "precision_4 = pred_2_precision.evaluate(cv_prediction_4)\n",
    "recall_4 = pred_4_recall.evaluate(cv_prediction_4)\n",
    "\n",
    "pred_5_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedPrecision')\n",
    "pred_5_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='weightedRecall')\n",
    "precision_5 = pred_5_precision.evaluate(cv_prediction_5)\n",
    "recall_5 = pred_5_recall.evaluate(cv_prediction_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_1: 0.8780\n",
      "Recall_1: 0.8863 \n",
      "\n",
      "Precision_2: 0.8705\n",
      "Recall_1: 0.8718\n",
      "\n",
      "Precision_3: 0.8999\n",
      "Recall_1: 0.9026\n",
      "\n",
      "Precision_4: 0.8954\n",
      "Recall_1: 0.9031\n",
      "\n",
      "Precision_5: 0.8977\n",
      "Recall_1: 0.9015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Precision_1: %0.4f' %precision_1)\n",
    "print('Recall_1: %0.4f \\n' %recall_1)\n",
    "\n",
    "print('Precision_2: %0.4f' %precision_2)\n",
    "print('Recall_1: %0.4f\\n' %recall_2)\n",
    "\n",
    "print('Precision_3: %0.4f' %precision_3)\n",
    "print('Recall_1: %0.4f\\n' %recall_3)\n",
    "\n",
    "print('Precision_4: %0.4f' %precision_4)\n",
    "print('Recall_1: %0.4f\\n' %recall_4)\n",
    "\n",
    "print('Precision_5: %0.4f' %precision_5)\n",
    "print('Recall_1: %0.4f\\n' %recall_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average for Precision is: 0.8883\n",
      "The average for Recall is: 0.8931\n"
     ]
    }
   ],
   "source": [
    "# mean results for precision and recall for each split data\n",
    "from statistics import mean\n",
    "nums_precision = [precision_1, precision_2, precision_3, precision_4, precision_5]\n",
    "avgP = mean(nums_precision)\n",
    "print('The average for Precision is: %0.4f' % avgP)\n",
    "\n",
    "nums_recall = [recall_1, recall_2, recall_3, recall_4, recall_5]\n",
    "avgR = mean(nums_recall)\n",
    "print('The average for Recall is: %0.4f' %avgR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e04e036266f305a312129840b06cfb353c1d2d005428a9f350cd8fe1c5454b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
